{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Satellite Image Classification using Deep Learning\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates multi-class image classification on satellite imagery using transfer learning. We'll classify land use patterns from the **EuroSAT dataset** which contains 27,000 labeled satellite images across 10 classes.\n",
    "\n",
    "### Classes:\n",
    "- Annual Crop\n",
    "- Forest\n",
    "- Herbaceous Vegetation\n",
    "- Highway\n",
    "- Industrial\n",
    "- Pasture\n",
    "- Permanent Crop\n",
    "- Residential\n",
    "- River\n",
    "- Sea/Lake\n",
    "\n",
    "### What we'll cover:\n",
    "1. Data loading and exploration\n",
    "2. Transfer learning with multiple architectures (ResNet50, EfficientNet, Vision Transformer)\n",
    "3. Training with data augmentation\n",
    "4. Model evaluation and comparison\n",
    "5. Grad-CAM visualization for interpretability\n",
    "6. Interactive demo with Gradio\n",
    "\n",
    "### Dataset Source:\n",
    "EuroSAT: Land Use and Land Cover Classification with Sentinel-2\n",
    "- Paper: https://arxiv.org/abs/1709.00029\n",
    "- Each image is 64x64 pixels, RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q timm  # PyTorch Image Models for pretrained models\n",
    "!pip install -q grad-cam  # For visualization\n",
    "!pip install -q gradio  # For demo interface\n",
    "!pip install -q scikit-learn matplotlib seaborn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import timm\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Grad-CAM\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-download"
   },
   "source": [
    "## 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-data"
   },
   "outputs": [],
   "source": [
    "# Download EuroSAT dataset\n",
    "# The dataset is available through torchvision or can be downloaded directly\n",
    "\n",
    "# Option 1: Download from source\n",
    "!wget -q http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
    "!unzip -q EuroSAT.zip\n",
    "!rm EuroSAT.zip\n",
    "\n",
    "# Set data directory\n",
    "data_dir = 'EuroSAT/2750'\n",
    "print(f\"Dataset downloaded to: {data_dir}\")\n",
    "\n",
    "# List classes\n",
    "classes = sorted(os.listdir(data_dir))\n",
    "print(f\"\\nNumber of classes: {len(classes)}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eda"
   },
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "count-images"
   },
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "class_counts = {}\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png'))])\n",
    "    class_counts[class_name] = count\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue', edgecolor='navy')\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Number of Images', fontsize=12)\n",
    "plt.title('EuroSAT Dataset: Class Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal images: {sum(class_counts.values())}\")\n",
    "print(f\"Images per class: {class_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-samples"
   },
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png'))]\n",
    "    sample_image = random.choice(image_files)\n",
    "    img_path = os.path.join(class_path, sample_image)\n",
    "    \n",
    "    img = plt.imread(img_path)\n",
    "    \n",
    "    ax = axes[idx // 5, idx % 5]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_name, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-prep"
   },
   "source": [
    "## 4. Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transforms"
   },
   "outputs": [],
   "source": [
    "# Define transforms for training and validation\n",
    "# Training: aggressive augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 for pretrained models\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Validation: minimal augmentation\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Transforms defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-datasets"
   },
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "full_dataset = datasets.ImageFolder(data_dir)\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "test_dataset.dataset.transform = val_transforms\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-def"
   },
   "source": [
    "## 5. Model Definition and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-function"
   },
   "outputs": [],
   "source": [
    "def create_model(model_name='resnet50', num_classes=10, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create a model using timm library.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model architecture\n",
    "        num_classes: Number of output classes\n",
    "        pretrained: Whether to use pretrained weights\n",
    "    \"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# Available models to compare\n",
    "available_models = {\n",
    "    'resnet50': 'ResNet-50',\n",
    "    'efficientnet_b0': 'EfficientNet-B0',\n",
    "    'vit_tiny_patch16_224': 'Vision Transformer (Tiny)'\n",
    "}\n",
    "\n",
    "print(\"Available models for training:\")\n",
    "for key, name in available_models.items():\n",
    "    print(f\"  - {name} ({key})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-function"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': running_loss/len(pbar), 'acc': 100.*correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss/len(pbar), 'acc': 100.*correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Complete training loop.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "    \n",
    "    return history, best_val_acc\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 6. Train Models\n",
    "\n",
    "We'll train multiple architectures and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-resnet"
   },
   "outputs": [],
   "source": [
    "# Train ResNet-50\n",
    "print(\"=\"*70)\n",
    "print(\"Training ResNet-50\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "resnet_model = create_model('resnet50', num_classes=10, pretrained=True)\n",
    "resnet_history, resnet_best_acc = train_model(\n",
    "    resnet_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=10,\n",
    "    device=device,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "\n",
    "print(f\"\\nResNet-50 Best Validation Accuracy: {resnet_best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-efficientnet"
   },
   "outputs": [],
   "source": [
    "# Train EfficientNet-B0\n",
    "print(\"=\"*70)\n",
    "print(\"Training EfficientNet-B0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "efficientnet_model = create_model('efficientnet_b0', num_classes=10, pretrained=True)\n",
    "efficientnet_history, efficientnet_best_acc = train_model(\n",
    "    efficientnet_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=10,\n",
    "    device=device,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "\n",
    "print(f\"\\nEfficientNet-B0 Best Validation Accuracy: {efficientnet_best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz-training"
   },
   "source": [
    "## 7. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-history"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(histories, model_names):\n",
    "    \"\"\"\n",
    "    Plot training history for multiple models.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    for history, name in zip(histories, model_names):\n",
    "        axes[0].plot(history['train_loss'], label=f'{name} Train', linestyle='-')\n",
    "        axes[0].plot(history['val_loss'], label=f'{name} Val', linestyle='--')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    for history, name in zip(histories, model_names):\n",
    "        axes[1].plot(history['train_acc'], label=f'{name} Train', linestyle='-')\n",
    "        axes[1].plot(history['val_acc'], label=f'{name} Val', linestyle='--')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison\n",
    "plot_training_history(\n",
    "    [resnet_history, efficientnet_history],\n",
    "    ['ResNet-50', 'EfficientNet-B0']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 8. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval-function"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval-best-model"
   },
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "best_model = create_model('efficientnet_b0', num_classes=10, pretrained=False)\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "test_acc, test_f1, test_preds, test_labels = evaluate_model(best_model, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gradcam"
   },
   "source": [
    "## 9. Grad-CAM Visualization\n",
    "\n",
    "Visualize what the model is looking at when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gradcam-function"
   },
   "outputs": [],
   "source": [
    "def visualize_gradcam(model, image_tensor, true_label, pred_label, class_names, device):\n",
    "    \"\"\"\n",
    "    Generate and visualize Grad-CAM.\n",
    "    \"\"\"\n",
    "    # Prepare model\n",
    "    model.eval()\n",
    "    \n",
    "    # Define target layer (last conv layer)\n",
    "    # For EfficientNet, it's typically the last conv layer before classifier\n",
    "    target_layers = [model.conv_head] if hasattr(model, 'conv_head') else [model.layer4[-1]]\n",
    "    \n",
    "    # Create Grad-CAM object\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    \n",
    "    # Generate CAM\n",
    "    targets = [ClassifierOutputTarget(pred_label)]\n",
    "    grayscale_cam = cam(input_tensor=image_tensor.unsqueeze(0), targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Denormalize image for visualization\n",
    "    img = image_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Generate visualization\n",
    "    visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Original Image', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(grayscale_cam, cmap='jet')\n",
    "    axes[1].set_title('Grad-CAM Heatmap', fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(visualization)\n",
    "    axes[2].set_title(f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]}', fontsize=12)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Grad-CAM for a few test samples\n",
    "test_dataset_subset = torch.utils.data.Subset(test_dataset, range(5))\n",
    "test_loader_subset = DataLoader(test_dataset_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"Grad-CAM Visualizations:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, (image, label) in enumerate(test_loader_subset):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = best_model(image)\n",
    "        _, pred = output.max(1)\n",
    "    \n",
    "    print(f\"\\nSample {idx+1}:\")\n",
    "    visualize_gradcam(\n",
    "        best_model, \n",
    "        image.squeeze(0), \n",
    "        label.item(), \n",
    "        pred.item(), \n",
    "        class_names, \n",
    "        device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo"
   },
   "source": [
    "## 10. Interactive Demo with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gradio-demo"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "def predict_satellite_image(image):\n",
    "    \"\"\"\n",
    "    Predict land use class from satellite image.\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {class_names[i]: float(probabilities[i]) for i in range(len(class_names))}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_satellite_image,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload Satellite Image\"),\n",
    "    outputs=gr.Label(num_top_classes=10, label=\"Predictions\"),\n",
    "    title=\"Satellite Image Classification\",\n",
    "    description=\"Upload a satellite image to classify land use type. The model was trained on the EuroSAT dataset.\",\n",
    "    examples=[\n",
    "        # Add paths to sample images here if desired\n",
    "    ],\n",
    "    theme=\"default\"\n",
    ")\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 11. Conclusions and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "- Successfully trained multiple deep learning models for satellite image classification\n",
    "- Achieved strong performance using transfer learning\n",
    "- Grad-CAM visualizations show the model focuses on relevant features\n",
    "\n",
    "### Model Comparison:\n",
    "Summary of model performances will be displayed after training.\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **More aggressive augmentation**: Try MixUp, CutMix, or AutoAugment\n",
    "2. **Larger models**: Test EfficientNet-B3/B4 or ViT-Base\n",
    "3. **Ensemble methods**: Combine predictions from multiple models\n",
    "4. **Class balancing**: Use weighted loss or oversampling if classes are imbalanced\n",
    "5. **Test-time augmentation**: Average predictions over multiple augmented versions\n",
    "6. **Fine-tune more layers**: Unfreeze earlier layers for longer training\n",
    "\n",
    "### Real-world Applications:\n",
    "- Urban planning and development monitoring\n",
    "- Agricultural monitoring and crop assessment\n",
    "- Environmental conservation and deforestation tracking\n",
    "- Disaster response and damage assessment\n",
    "- Infrastructure planning\n",
    "\n",
    "### Deployment Options:\n",
    "- Create REST API with Flask/FastAPI\n",
    "- Deploy on cloud platforms (AWS, GCP, Azure)\n",
    "- Mobile app integration\n",
    "- Real-time satellite feed processing\n",
    "\n",
    "---\n",
    "\n",
    "**Project completed successfully!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-notebook"
   },
   "source": [
    "## 12. Save Results and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": [
    "# Save model comparison results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'ResNet-50': {\n",
    "        'best_val_acc': resnet_best_acc,\n",
    "        'final_train_acc': resnet_history['train_acc'][-1],\n",
    "        'final_val_acc': resnet_history['val_acc'][-1]\n",
    "    },\n",
    "    'EfficientNet-B0': {\n",
    "        'best_val_acc': efficientnet_best_acc,\n",
    "        'final_train_acc': efficientnet_history['train_acc'][-1],\n",
    "        'final_val_acc': efficientnet_history['val_acc'][-1]\n",
    "    },\n",
    "    'test_accuracy': test_acc * 100,\n",
    "    'test_f1_score': test_f1\n",
    "}\n",
    "\n",
    "with open('model_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to model_results.json\")\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Download Results and Visualizations\n\nSave all visualizations and create a downloadable package of your results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create output directory for visualizations\nimport os\nfrom datetime import datetime\n\noutput_dir = 'satellite_classification_results'\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(f'{output_dir}/visualizations', exist_ok=True)\nos.makedirs(f'{output_dir}/models', exist_ok=True)\n\nprint(f\"Created output directory: {output_dir}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save all visualizations as high-quality images\nprint(\"Saving visualizations...\")\nprint(\"=\" * 70)\n\n# 1. Class Distribution\nprint(\"\\n1. Class distribution chart...\")\nplt.figure(figsize=(12, 6))\nplt.bar(class_counts.keys(), class_counts.values(), color='skyblue', edgecolor='navy')\nplt.xlabel('Class', fontsize=12)\nplt.ylabel('Number of Images', fontsize=12)\nplt.title('EuroSAT Dataset: Class Distribution', fontsize=14, fontweight='bold')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.grid(axis='y', alpha=0.3)\nplt.savefig(f'{output_dir}/visualizations/class_distribution.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"   ‚úì Saved: class_distribution.png\")\n\n# 2. Sample Images Grid\nprint(\"\\n2. Sample images grid...\")\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold')\nfor idx, class_name in enumerate(classes):\n    class_path = os.path.join(data_dir, class_name)\n    image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png'))]\n    sample_image = image_files[0]\n    img_path = os.path.join(class_path, sample_image)\n    img = plt.imread(img_path)\n    ax = axes[idx // 5, idx % 5]\n    ax.imshow(img)\n    ax.set_title(class_name, fontsize=10)\n    ax.axis('off')\nplt.tight_layout()\nplt.savefig(f'{output_dir}/visualizations/sample_images.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"   ‚úì Saved: sample_images.png\")\n\n# 3. Training History\nprint(\"\\n3. Training history plots...\")\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\nfor history, name in zip([resnet_history, efficientnet_history], ['ResNet-50', 'EfficientNet-B0']):\n    axes[0].plot(history['train_loss'], label=f'{name} Train', linestyle='-', linewidth=2)\n    axes[0].plot(history['val_loss'], label=f'{name} Val', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Loss', fontsize=12)\naxes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\nfor history, name in zip([resnet_history, efficientnet_history], ['ResNet-50', 'EfficientNet-B0']):\n    axes[1].plot(history['train_acc'], label=f'{name} Train', linestyle='-', linewidth=2)\n    axes[1].plot(history['val_acc'], label=f'{name} Val', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Accuracy (%)', fontsize=12)\naxes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig(f'{output_dir}/visualizations/training_history.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"   ‚úì Saved: training_history.png\")\n\n# 4. Confusion Matrix\nprint(\"\\n4. Confusion matrix...\")\ncm = confusion_matrix(test_labels, test_preds)\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix - Best Model', fontsize=16, fontweight='bold')\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig(f'{output_dir}/visualizations/confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"   ‚úì Saved: confusion_matrix.png\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úì All visualizations saved!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save Grad-CAM visualizations\nprint(\"\\nSaving Grad-CAM visualizations...\")\nprint(\"=\" * 70)\n\ntest_dataset_subset = torch.utils.data.Subset(test_dataset, range(5))\ntest_loader_subset = DataLoader(test_dataset_subset, batch_size=1, shuffle=False)\n\nfor idx, (image, label) in enumerate(test_loader_subset):\n    image = image.to(device)\n    with torch.no_grad():\n        output = best_model(image)\n        _, pred = output.max(1)\n    \n    # Generate Grad-CAM\n    best_model.eval()\n    target_layers = [best_model.conv_head] if hasattr(best_model, 'conv_head') else [best_model.layer4[-1]]\n    cam = GradCAM(model=best_model, target_layers=target_layers)\n    targets = [ClassifierOutputTarget(pred.item())]\n    grayscale_cam = cam(input_tensor=image, targets=targets)\n    grayscale_cam = grayscale_cam[0, :]\n    \n    # Denormalize image\n    img = image.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    img = std * img + mean\n    img = np.clip(img, 0, 1)\n    \n    # Create visualization\n    visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n    \n    # Save plot\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    axes[0].imshow(img)\n    axes[0].set_title('Original Image', fontsize=12)\n    axes[0].axis('off')\n    axes[1].imshow(grayscale_cam, cmap='jet')\n    axes[1].set_title('Grad-CAM Heatmap', fontsize=12)\n    axes[1].axis('off')\n    axes[2].imshow(visualization)\n    axes[2].set_title(f'True: {class_names[label.item()]}\\nPred: {class_names[pred.item()]}', fontsize=12)\n    axes[2].axis('off')\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/visualizations/gradcam_sample_{idx+1}.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"   ‚úì Saved: gradcam_sample_{idx+1}.png\")\n\nprint(\"\\n\" + \"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create comprehensive summary report\nprint(\"\\nCreating summary report...\")\nprint(\"=\" * 70)\n\nreport = f\"\"\"\nSATELLITE IMAGE CLASSIFICATION - PROJECT REPORT\n{'=' * 80}\n\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nPROJECT OVERVIEW\n{'-' * 80}\nDataset: EuroSAT (Sentinel-2 Satellite Images)\nTotal Images: {sum(class_counts.values())}\nNumber of Classes: {len(class_names)}\nImage Size: 64x64 pixels (resized to 224x224 for training)\n\nDATASET SPLIT\n{'-' * 80}\nTraining samples: {len(train_dataset)} ({len(train_dataset)/len(full_dataset)*100:.1f}%)\nValidation samples: {len(val_dataset)} ({len(val_dataset)/len(full_dataset)*100:.1f}%)\nTest samples: {len(test_dataset)} ({len(test_dataset)/len(full_dataset)*100:.1f}%)\n\nCLASSES\n{'-' * 80}\n\"\"\"\n\nfor i, class_name in enumerate(class_names, 1):\n    report += f\"{i:2d}. {class_name:25s} - {class_counts[class_name]:5d} images\\n\"\n\nreport += f\"\"\"\n\nMODEL ARCHITECTURES TRAINED\n{'-' * 80}\n1. ResNet-50 (25.6M parameters)\n2. EfficientNet-B0 (5.3M parameters)\n\nTRAINING CONFIGURATION\n{'-' * 80}\nOptimizer: Adam\nLearning Rate: 0.0001\nScheduler: ReduceLROnPlateau\nEpochs: 10\nBatch Size: {batch_size}\nData Augmentation:\n  - Random Horizontal/Vertical Flip\n  - Random Rotation (¬±30¬∞)\n  - Color Jitter\n  - Random Affine Transform\n\nRESULTS\n{'=' * 80}\n\nResNet-50 Performance:\n{'-' * 80}\nBest Validation Accuracy: {resnet_best_acc:.2f}%\nFinal Training Accuracy: {resnet_history['train_acc'][-1]:.2f}%\nFinal Validation Accuracy: {resnet_history['val_acc'][-1]:.2f}%\n\nEfficientNet-B0 Performance:\n{'-' * 80}\nBest Validation Accuracy: {efficientnet_best_acc:.2f}%\nFinal Training Accuracy: {efficientnet_history['train_acc'][-1]:.2f}%\nFinal Validation Accuracy: {efficientnet_history['val_acc'][-1]:.2f}%\n\nTest Set Performance (Best Model):\n{'-' * 80}\nTest Accuracy: {test_acc*100:.2f}%\nWeighted F1-Score: {test_f1:.4f}\n\nDETAILED CLASSIFICATION REPORT\n{'=' * 80}\n{classification_report(test_labels, test_preds, target_names=class_names)}\n\nFILES GENERATED\n{'-' * 80}\nModels:\n  - best_model.pth (Best performing model weights)\n\nVisualizations:\n  - class_distribution.png\n  - sample_images.png\n  - training_history.png\n  - confusion_matrix.png\n  - gradcam_sample_1.png through gradcam_sample_5.png\n\nData:\n  - model_results.json (Detailed metrics)\n  - project_summary.txt (This report)\n\n{'=' * 80}\nEnd of Report\n\"\"\"\n\n# Save report\nwith open(f'{output_dir}/project_summary.txt', 'w') as f:\n    f.write(report)\n\nprint(\"   ‚úì Saved: project_summary.txt\")\nprint(\"\\n\" + \"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Copy model files and create zip archive\nprint(\"\\nCopying model files...\")\nimport shutil\nimport zipfile\n\n# Copy files\nif os.path.exists('best_model.pth'):\n    shutil.copy('best_model.pth', f'{output_dir}/models/best_model.pth')\n    print(\"   ‚úì Copied: best_model.pth\")\n\nif os.path.exists('model_results.json'):\n    shutil.copy('model_results.json', f'{output_dir}/model_results.json')\n    print(\"   ‚úì Copied: model_results.json\")\n\n# Create zip archive\nprint(\"\\nCreating zip archive...\")\nzip_filename = 'satellite_classification_results.zip'\n\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, os.path.dirname(output_dir))\n            zipf.write(file_path, arcname)\n\nzip_size = os.path.getsize(zip_filename) / (1024 * 1024)\nprint(f\"   ‚úì Created: {zip_filename} ({zip_size:.2f} MB)\")\nprint(\"\\n\" + \"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display file inventory\nprint(\"\\nFile Inventory:\")\nprint(\"=\" * 70)\n\ntotal_size = 0\nfile_list = []\n\nfor root, dirs, files in os.walk(output_dir):\n    for file in files:\n        filepath = os.path.join(root, file)\n        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n        total_size += size_mb\n        rel_path = os.path.relpath(filepath, output_dir)\n        file_list.append((rel_path, size_mb))\n\n# Sort and display\nfile_list.sort()\nfor filename, size in file_list:\n    print(f\"  {filename:50s} {size:8.2f} MB\")\n\nprint(f\"\\n{'Total Size:':50s} {total_size:8.2f} MB\")\nprint(f\"{'Zip Archive:':50s} {zip_size:8.2f} MB\")\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úì ALL RESULTS PACKAGED SUCCESSFULLY!\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Download Complete Package (Google Colab)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Download complete zip archive (Colab only)\ntry:\n    from google.colab import files\n    print(\"Downloading complete results package...\")\n    print(f\"File: {zip_filename} ({zip_size:.2f} MB)\")\n    files.download(zip_filename)\n    print(\"‚úì Download started!\")\nexcept ImportError:\n    print(\"Not running in Google Colab.\")\n    print(f\"\\nAll files saved locally:\")\n    print(f\"  - Directory: {output_dir}/\")\n    print(f\"  - Zip file: {zip_filename}\")\nexcept Exception as e:\n    print(f\"Download error: {e}\")\n    print(f\"File available at: {zip_filename}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Download Individual Files (Optional)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Download individual files (uncomment what you need)\ntry:\n    from google.colab import files\n    \n    # Download best model\n    # files.download(f'{output_dir}/models/best_model.pth')\n    \n    # Download summary report\n    # files.download(f'{output_dir}/project_summary.txt')\n    \n    # Download specific visualizations\n    # files.download(f'{output_dir}/visualizations/training_history.png')\n    # files.download(f'{output_dir}/visualizations/confusion_matrix.png')\n    \n    print(\"Uncomment the files you want to download and run this cell again!\")\n    \nexcept ImportError:\n    print(f\"Files available locally at: {output_dir}/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## What's Included in Your Download Package\n\n**Complete Package (`satellite_classification_results.zip`):**\n\nüìÅ **Models**\n- `best_model.pth` - Trained EfficientNet-B0 weights (~20 MB)\n\nüìÅ **Visualizations** (High-resolution PNG, 300 DPI)\n- `class_distribution.png` - Dataset class balance chart\n- `sample_images.png` - Grid of sample images from all 10 classes  \n- `training_history.png` - Loss and accuracy curves for both models\n- `confusion_matrix.png` - Test set confusion matrix heatmap\n- `gradcam_sample_1.png` through `gradcam_sample_5.png` - Model interpretability visualizations\n\nüìÅ **Reports & Data**\n- `project_summary.txt` - Comprehensive text report with all metrics\n- `model_results.json` - Machine-readable results file\n\n### Using Your Downloaded Model\n\n```python\nimport torch\nimport timm\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load model\nmodel = timm.create_model('efficientnet_b0', pretrained=False, num_classes=10)\nmodel.load_state_dict(torch.load('best_model.pth', map_location='cpu'))\nmodel.eval()\n\n# Predict on new image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimage = Image.open('your_satellite_image.jpg')\ninput_tensor = transform(image).unsqueeze(0)\noutput = model(input_tensor)\nprediction = output.argmax(1).item()\n```\n\n### Perfect For\n- Portfolio presentations\n- GitHub repository\n- Project documentation\n- Academic submissions\n- Job applications\n\n---\n\n**üéâ Congratulations! Your satellite image classification project is complete!**",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}